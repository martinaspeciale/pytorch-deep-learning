{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f688ed",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-05T14:47:36.983419Z",
     "iopub.status.busy": "2025-09-05T14:47:36.983097Z",
     "iopub.status.idle": "2025-09-05T14:47:51.904855Z",
     "shell.execute_reply": "2025-09-05T14:47:51.903877Z"
    },
    "papermill": {
     "duration": 14.928722,
     "end_time": "2025-09-05T14:47:51.907009",
     "exception": false,
     "start_time": "2025-09-05T14:47:36.978287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 52.2MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.71MB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.7MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.20MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "\n",
    "train_mnist = torchvision.datasets.MNIST(\n",
    "    \"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_mnist = torchvision.datasets.MNIST(\n",
    "    \"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4cb3c8",
   "metadata": {
    "papermill": {
     "duration": 0.003453,
     "end_time": "2025-09-05T14:47:51.914666",
     "exception": false,
     "start_time": "2025-09-05T14:47:51.911213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model: AutoEncoder (2-D bottleneck)\n",
    "### What is an AutoEncoder (AE)?\n",
    "\n",
    "An **AutoEncoder** learns to:\n",
    "1) **Encode** an input \\(x\\) into a low-dimensional **latent vector** \\(z\\) (the **bottleneck**), and\n",
    "2) **Decode** \\(z\\) back into a reconstruction \\($\\hat{x}\\$) that resembles the original.\n",
    "\n",
    "For MNIST (28×28 grayscale), we’ll use a **fully-connected (MLP: MultiLayer Perceptron) AE** with a **2-D bottleneck**:\n",
    "- **Why 2-D?** So we can **visualize** the latent space later as a 2D scatter plot (each image → one point).\n",
    "- **Encoder**: $(784 \\rightarrow 300 \\rightarrow 2)$\n",
    "- **Decoder**: $(2 \\rightarrow 300 \\rightarrow 784)$\n",
    "\n",
    "We keep it minimal:\n",
    "- **Activations**: `LeakyReLU` after the hidden layers (helps gradients with small negative slope).\n",
    "- **No output activation** on the decoder’s last layer because inputs are **standardized** (Mean=0.1307, Std=0.3081).  \n",
    "  If you were using raw \\([0,1]\\) pixels, a `Sigmoid` could be sensible; with standardized inputs, an unrestricted linear output works well with L1/L2 losses.\n",
    "\n",
    "---\n",
    "\n",
    "### Shape flow\n",
    "\n",
    "- Input batch \\(x\\): `[B, 1, 28, 28]`\n",
    "- **Flatten** to `[B, 784]` before feeding the encoder.\n",
    "- Encoder outputs \\(z\\): `[B, 2]`\n",
    "- Decoder maps back to `[B, 784]`\n",
    "- **Unflatten** to `[B, 1, 28, 28]` to compare against the original images.\n",
    "\n",
    "This ensures the training loss can be computed as `loss(x, x_hat)` element-wise.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods we implement\n",
    "\n",
    "- `encode(x) → z`: only the encoder path (used later to visualize latents).\n",
    "- `decode(z) → x_hat`: only the decoder path (used later to manipulate latents and decode).\n",
    "- `forward(x) → x_hat`: full AE (encode then decode).\n",
    "\n",
    "---\n",
    "\n",
    "### Why `LeakyReLU`?\n",
    "\n",
    "- Like ReLU but with a small slope on negatives (mitigates “dying ReLUs”).\n",
    "- Often improves stability for simple MLP autoencoders.\n",
    "\n",
    "---\n",
    "\n",
    "### Sanity check idea (optional)\n",
    "\n",
    "After defining the model, run a tiny batch through it and verify:\n",
    "- `x.shape == recon.shape == [B, 1, 28, 28]`\n",
    "- `features.shape == [B, 2]`\n",
    "\n",
    "We’ll do that right after we instantiate the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb9542",
   "metadata": {
    "papermill": {
     "duration": 0.003349,
     "end_time": "2025-09-05T14:47:51.921579",
     "exception": false,
     "start_time": "2025-09-05T14:47:51.918230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Why is it called a *bottleneck*?\n",
    "\n",
    "In an autoencoder, the **bottleneck** is the **narrowest layer** in the middle — the smallest representation `z` through which all information about the input `x` must pass. By forcing data through this tight “neck,” the model must **compress** the input and keep only its most salient factors.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "x \\in \\mathbb{R}^{784} \\xrightarrow{\\text{Encoder}} z \\in \\mathbb{R}^{d} \\xrightarrow{\\text{Decoder}} \\hat{x} \\in \\mathbb{R}^{784} \\\\\n",
    "\\text{(wide)} \\quad \\longrightarrow \\quad \\text{(narrow)} \\quad \\longrightarrow \\quad \\text{(wide)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### What the bottleneck does\n",
    "- **Compression:** reduces dimensionality so the model captures key structure rather than copying pixels.  \n",
    "- **Architectural regularization:** discourages the trivial identity mapping (especially when `d << D`).  \n",
    "- **Generalization:** encourages discarding noise or redundancy and retaining meaningful features.\n",
    "\n",
    "---\n",
    "\n",
    "### Compression factor\n",
    "\n",
    "Let `D` be the input dimensionality (e.g., `28 × 28 = 784` for MNIST) and `d` the bottleneck size.\n",
    "\n",
    "$$\n",
    "\\text{compression factor} = \\frac{D}{d}\n",
    "$$\n",
    "\n",
    "**Examples**\n",
    "- `d = 2`  → \\( \\frac{784}{2} = 392\\times \\) compression (extreme; great for visualization).  \n",
    "- `d = 64` → \\( \\frac{784}{64} \\approx 12.25\\times \\) compression (typically better reconstructions).\n",
    "\n",
    "---\n",
    "\n",
    "### Choosing the bottleneck size `d` (trade-offs)\n",
    "- **Too small `d`** → heavy compression → risk of **underfitting** (blurry or poor reconstructions).  \n",
    "- **Too large `d`** → weak compression → risk of near **identity mapping** (little abstraction).  \n",
    "- **Balanced `d`** → good reconstruction quality **and** compact, structured latents.\n",
    "\n",
    "---\n",
    "\n",
    "### Undercomplete vs. Overcomplete\n",
    "- **Undercomplete** (`d < D`): a true *bottleneck*; capacity is limited by the latent dimensionality.  \n",
    "- **Overcomplete** (`d ≥ D`): no geometric bottleneck; to avoid identity mapping you need extra regularization, e.g.:  \n",
    "  - **Sparsity** penalties (e.g., L1 on activations),  \n",
    "  - **Denoising** (corrupt input, reconstruct clean),  \n",
    "  - **Contractive/variational** objectives (e.g., VAE).\n",
    "\n",
    "---\n",
    "\n",
    "### Why use a 2-D bottleneck in demos?\n",
    "A 2-D latent `z` can be **plotted directly**: each image becomes a point in 2D, typically forming clusters by digit class. This makes the learned representation easy to interpret and debug.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17bbb52d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T14:47:51.930378Z",
     "iopub.status.busy": "2025-09-05T14:47:51.929933Z",
     "iopub.status.idle": "2025-09-05T14:47:51.977641Z",
     "shell.execute_reply": "2025-09-05T14:47:51.976601Z"
    },
    "papermill": {
     "duration": 0.054225,
     "end_time": "2025-09-05T14:47:51.979280",
     "exception": false,
     "start_time": "2025-09-05T14:47:51.925055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape: torch.Size([4, 2])\n",
      "recon shape: torch.Size([4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, bottleneck_dim: int = 2):\n",
    "        super().__init__()\n",
    "        # Flatten image to vector and back\n",
    "        self.flatten   = nn.Flatten()                          # [B, 1, 28, 28] -> [B, 784]\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(1, 28, 28))  # [B, 784] -> [B, 1, 28, 28]\n",
    "\n",
    "        # Encoder: 784 -> 300 -> bottleneck_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 300),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(300, bottleneck_dim)\n",
    "        )\n",
    "\n",
    "        # Decoder: bottleneck_dim -> 300 -> 784\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dim, 300),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(300, 28*28)\n",
    "            # No final activation since inputs are standardized; use L1/L2 loss.\n",
    "            # If using raw [0,1] pixels, consider nn.Sigmoid() here.\n",
    "        )\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.flatten(x)       # [B, 1, 28, 28] -> [B, 784]\n",
    "        z = self.encoder(x)       # [B, 784] -> [B, bottleneck_dim]\n",
    "        return z\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        x_hat = self.decoder(z)   # [B, bottleneck_dim] -> [B, 784]\n",
    "        x_hat = self.unflatten(x_hat)  # [B, 784] -> [B, 1, 28, 28]\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decode(self.encode(x))\n",
    "\n",
    "\n",
    "# (Optional) quick shape sanity check\n",
    "if __name__ == \"__main__\":\n",
    "    model = AutoEncoder(bottleneck_dim=2)\n",
    "    dummy = torch.randn(4, 1, 28, 28)     # batch of 4\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(dummy)           # [4, 2]\n",
    "        recon = model(dummy)              # [4, 1, 28, 28]\n",
    "    print(\"z shape:\", z.shape)\n",
    "    print(\"recon shape:\", recon.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0c061",
   "metadata": {
    "papermill": {
     "duration": 0.004664,
     "end_time": "2025-09-05T14:47:51.988126",
     "exception": false,
     "start_time": "2025-09-05T14:47:51.983462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Why `Flatten` and `Unflatten` in an MLP AutoEncoder?\n",
    "\n",
    "**Problem:** MNIST images arrive as 4D tensors `[B, C, H, W] = [B, 1, 28, 28]`, but `nn.Linear` layers expect 2D inputs `[B, F]`.\n",
    "\n",
    "**Solution:**\n",
    "- `nn.Flatten()` converts each image into a feature vector:\n",
    "  $$\n",
    "  [B, 1, 28, 28] \\;\\rightarrow\\; [B, 28\\cdot28] = [B, 784]\n",
    "  $$\n",
    "  so the encoder/decoder (which are MLPs) can operate on 2D tensors.\n",
    "\n",
    "- `nn.Unflatten(dim=1, unflattened_size=(1, 28, 28))` reshapes the decoder output \n",
    "  $$\n",
    "  [B, 784] \\;\\rightarrow\\; [B, 1, 28, 28]\n",
    "  $$\n",
    "  so the reconstruction $(\\hat{x})$ has the **same shape** as the input \\(x\\).  \n",
    "  This is required to compute pixel-wise losses (L1/MSE) and to **plot** the result as an image.\n",
    "\n",
    "**When would you *not* need these?**\n",
    "- In a **convolutional autoencoder** (Conv2d/ConvTranspose2d), you keep the data as `[B, C, H, W]` throughout, so no flatten/unflatten is needed.\n",
    "\n",
    "**Why `dim=1` in `Unflatten`?**\n",
    "- After `Flatten`, your tensor is `[B, 784]`:  \n",
    "  - `dim=0` is the **batch**.  \n",
    "  - `dim=1` is the **feature** dimension we want to reshape back into `(C, H, W)`.\n",
    "\n",
    "**Alternatives**\n",
    "- You could use `x = x.view(B, -1)` or `x = x.reshape(B, -1)` and the inverse reshape manually.  \n",
    "  Using `nn.Flatten`/`nn.Unflatten` makes shapes explicit and shows up in `model` printouts for readability.\n",
    "\n",
    "- `Flatten` lets MLPs consume images.  \n",
    "- `Unflatten` restores image shape for loss/visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5967fe",
   "metadata": {
    "papermill": {
     "duration": 0.003566,
     "end_time": "2025-09-05T14:47:51.996236",
     "exception": false,
     "start_time": "2025-09-05T14:47:51.992670",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### sanity-check snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968a3cef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T14:47:52.005359Z",
     "iopub.status.busy": "2025-09-05T14:47:52.005033Z",
     "iopub.status.idle": "2025-09-05T14:47:52.014075Z",
     "shell.execute_reply": "2025-09-05T14:47:52.013042Z"
    },
    "papermill": {
     "duration": 0.015586,
     "end_time": "2025-09-05T14:47:52.015643",
     "exception": false,
     "start_time": "2025-09-05T14:47:52.000057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28]) -> torch.Size([4, 784]) -> torch.Size([4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "B = 4\n",
    "x = torch.randn(B, 1, 28, 28)\n",
    "\n",
    "flat = torch.nn.Flatten()\n",
    "unflat = torch.nn.Unflatten(dim=1, unflattened_size=(1, 28, 28))\n",
    "\n",
    "x_vec = flat(x)                 # [B, 784]\n",
    "# pretend the decoder produced a vector of the same size\n",
    "x_hat_vec = x_vec.clone()       \n",
    "x_hat = unflat(x_hat_vec)       # [B, 1, 28, 28]\n",
    "\n",
    "print(x.shape, \"->\", x_vec.shape, \"->\", x_hat.shape)\n",
    "# torch.Size([4, 1, 28, 28]) -> torch.Size([4, 784]) -> torch.Size([4, 1, 28, 28])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46bf433",
   "metadata": {
    "papermill": {
     "duration": 0.00414,
     "end_time": "2025-09-05T14:47:52.023728",
     "exception": false,
     "start_time": "2025-09-05T14:47:52.019588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training an AutoEncoder\n",
    "\n",
    "**Objective:** learn parameters so the decoder reconstructs the input from its latent \\( z \\).\n",
    "\n",
    "We minimize a **reconstruction loss** between the input $( x )$ and output $( \\hat{x} )$:\n",
    "$$\n",
    "\\mathcal{L}(x, \\hat{x}) =\n",
    "\\begin{cases}\n",
    "\\|x - \\hat{x}\\|_1 & \\text{(L1 / MAE)} \\\\\n",
    "\\|x - \\hat{x}\\|_2^2 & \\text{(MSE)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Choices (mirroring original notebook)**\n",
    "- **Loss:** `nn.L1Loss()` (robust to outliers; often yields sharper reconstructions)\n",
    "- **Optimizer:** `Adam(lr=1e-3)` (good default)\n",
    "- **Epochs:** 3 (increase if underfitting)\n",
    "- **Batch size:** 32\n",
    "- **Device:** use GPU if available for speed\n",
    "\n",
    "**Loop per batch**\n",
    "1. `recon = model(x)` — forward pass (encode → decode)  \n",
    "2. `loss = loss_fn(x, recon)` — compute reconstruction loss  \n",
    "3. `optimizer.zero_grad()` — clear stale gradients  \n",
    "4. `loss.backward()` — backpropagate  \n",
    "5. `optimizer.step()` — update weights\n",
    "\n",
    "**Training vs. Eval modes**\n",
    "- `model.train()` enables training behavior (e.g., dropout/batchnorm updates).\n",
    "- `model.eval()` disables those behaviors. We’ll use it later during inference/visualization.\n",
    "\n",
    "**Sanity tips**\n",
    "- Track and print the **mean epoch loss** to ensure it’s decreasing.\n",
    "- If using GPU, remember: plotting/NumPy needs **CPU** tensors → call `.cpu()` before `.numpy()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ee08a",
   "metadata": {
    "papermill": {
     "duration": 0.003431,
     "end_time": "2025-09-05T14:47:52.030865",
     "exception": false,
     "start_time": "2025-09-05T14:47:52.027434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**naming convention** for batches:\n",
    "- `xb` = “**x batch**” (a tensor of many images)\n",
    "- `yb` = “**y batch**” (a tensor of many labels)\n",
    "\n",
    "\n",
    "### In an AutoEncoder\n",
    "An AE learns to reconstruct its input, so the **target is the input**.\n",
    "That’s why you often see the label **ignored** with `_`:\n",
    "\n",
    "```python\n",
    "for xb, _ in dl:           # labels unused in AE training\n",
    "    recon = model(xb)\n",
    "    loss = loss_fn(xb, recon) # target == input\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b6b422",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T14:47:52.040440Z",
     "iopub.status.busy": "2025-09-05T14:47:52.039360Z",
     "iopub.status.idle": "2025-09-05T14:49:12.795060Z",
     "shell.execute_reply": "2025-09-05T14:49:12.793865Z"
    },
    "papermill": {
     "duration": 80.762352,
     "end_time": "2025-09-05T14:49:12.796875",
     "exception": false,
     "start_time": "2025-09-05T14:47:52.034523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: mean L1 loss = 0.337823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: mean L1 loss = 0.310739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: mean L1 loss = 0.302827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Model + optimizer + loss\n",
    "model = AutoEncoder(bottleneck_dim=2).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "# DataLoader\n",
    "dl = torch.utils.data.DataLoader(train_mnist, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train\n",
    "EPOCHS = 3\n",
    "model.train()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(dl, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n",
    "    for xb, _ in pbar:\n",
    "        xb = xb.to(DEVICE)                 # [B, 1, 28, 28]\n",
    "        recon = model(xb)                  # [B, 1, 28, 28]\n",
    "        loss = loss_fn(xb, recon)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        total += xb.size(0)\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    mean_loss = running_loss / total\n",
    "    print(f\"Epoch {epoch}: mean L1 loss = {mean_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d02dac",
   "metadata": {
    "papermill": {
     "duration": 0.344075,
     "end_time": "2025-09-05T14:49:13.467235",
     "exception": false,
     "start_time": "2025-09-05T14:49:13.123160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Single-sample encode/decode + latent manipulation\n",
    "### Goal\n",
    "Understand what the 2-D latent space represents by:\n",
    "1) **Encoding** one test image $(x)$ into its latent $(z)$,\n",
    "2) **Decoding** to reconstruct $(\\hat{x})$,\n",
    "3) **Manipulating** $(z)$ (e.g., set \\(z=[1,1]\\)) and decoding again to see how the output changes.\n",
    "\n",
    "### Key details\n",
    "- **Batch dim:** The model expects batches. A single image `[1, 28, 28]` must become `[1, 1, 28, 28]`. We use `unsqueeze(0)` to add the batch dimension.\n",
    "- **Eval mode:** `model.eval()` disables training-only behavior (good hygiene even if we don’t use dropout/batchnorm).\n",
    "- **No gradients:** `with torch.no_grad():` avoids tracking gradients during inference.\n",
    "- **Device:** Move tensors to the same device as the model; bring back to CPU for plotting (`.cpu()`).\n",
    "- **Unnormalize for display:** We normalized inputs during loading (mean=0.1307, std=0.3081). To view images as typical grayscale, invert the transform:\n",
    "  $$\n",
    "  x_{\\text{vis}} = \\mathrm{clamp}(x \\cdot 0.3081 + 0.1307,\\, 0,\\, 1)\n",
    "  $$\n",
    "\n",
    "### What to look for\n",
    "- The **reconstruction** $(\\hat{x})$ should look like the original digit (not perfect — this is a tiny AE).\n",
    "- The **manipulated latent** often changes style/shape; with a 2-D $(z)$, moving along each axis tends to adjust some interpretable quality (e.g., stroke thickness, slant).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b919339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T14:49:14.191146Z",
     "iopub.status.busy": "2025-09-05T14:49:14.190693Z",
     "iopub.status.idle": "2025-09-05T14:49:14.529904Z",
     "shell.execute_reply": "2025-09-05T14:49:14.528962Z"
    },
    "papermill": {
     "duration": 0.656633,
     "end_time": "2025-09-05T14:49:14.532522",
     "exception": false,
     "start_time": "2025-09-05T14:49:13.875889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original z: [[-1.3443823  2.2305467]]\n",
      "Edited   z: [[1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper to unnormalize a batch [B, 1, 28, 28] for display\n",
    "def unnormalize(img_batch: torch.Tensor, mean=0.1307, std=0.3081):\n",
    "    x = img_batch * std + mean\n",
    "    return x.clamp(0, 1)\n",
    "\n",
    "# Pick one test sample\n",
    "x, y = test_mnist[0]          # x: [1, 28, 28], y: scalar label\n",
    "x = x.unsqueeze(0)            # [1, 1, 28, 28] add batch dimension\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Move to device\n",
    "    x_device = x.to(DEVICE)\n",
    "\n",
    "    # Encode -> z in R^2\n",
    "    z = model.encode(x_device)            # [1, 2]\n",
    "\n",
    "    # Reconstruct original\n",
    "    x_recon = model.decode(z)             # [1, 1, 28, 28]\n",
    "\n",
    "    # Manipulate latent (example: set z = [1, 1])\n",
    "    z_edit = z.clone()\n",
    "    z_edit[:, 0] = 1.0\n",
    "    z_edit[:, 1] = 1.0\n",
    "\n",
    "    x_recon_edit = model.decode(z_edit)   # [1, 1, 28, 28]\n",
    "\n",
    "# Bring to CPU for plotting and unnormalize for nicer display\n",
    "x_vis         = unnormalize(x.cpu())\n",
    "x_recon_vis   = unnormalize(x_recon.cpu())\n",
    "x_edit_vis    = unnormalize(x_recon_edit.cpu())\n",
    "\n",
    "# Plot original, reconstruction, and edited-latent reconstruction\n",
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n",
    "axs[0].imshow(x_vis[0, 0].numpy(), cmap=\"gray\")\n",
    "axs[0].set_title(f\"Original (y={y})\")\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "axs[1].imshow(x_recon_vis[0, 0].numpy(), cmap=\"gray\")\n",
    "axs[1].set_title(\"Reconstruction\")\n",
    "axs[1].axis(\"off\")\n",
    "\n",
    "axs[2].imshow(x_edit_vis[0, 0].numpy(), cmap=\"gray\")\n",
    "axs[2].set_title(\"Decode z=[1,1]\")\n",
    "axs[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Original z:\", z.cpu().numpy())\n",
    "print(\"Edited   z:\", z_edit.cpu().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 105.949068,
   "end_time": "2025-09-05T14:49:16.877069",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-05T14:47:30.928001",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
