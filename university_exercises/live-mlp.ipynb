{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aede23a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-06T09:58:13.909123Z",
     "iopub.status.busy": "2025-09-06T09:58:13.908751Z",
     "iopub.status.idle": "2025-09-06T09:58:25.787391Z",
     "shell.execute_reply": "2025-09-06T09:58:25.786401Z"
    },
    "papermill": {
     "duration": 11.885666,
     "end_time": "2025-09-06T09:58:25.789217",
     "exception": false,
     "start_time": "2025-09-06T09:58:13.903551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbcd026",
   "metadata": {
    "papermill": {
     "duration": 0.002735,
     "end_time": "2025-09-06T09:58:25.795294",
     "exception": false,
     "start_time": "2025-09-06T09:58:25.792559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MLP for MNIST Classification\n",
    "\n",
    "This notebook trains a **Multilayer Perceptron (MLP)** to classify **MNIST** digits (0–9). Below is a concise theory guide to frame each step of the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Problem setup\n",
    "\n",
    "- **Inputs**: grayscale images $x ∈ ℝ^{1×28×28}$ (values in `[0,1]` after `ToTensor`).\n",
    "- **Flattening**: we reshape to a vector $x_{vec} ∈ ℝ^{784}$ (since `28×28=784`) before feeding the MLP.\n",
    "- **Targets**: integer class labels `y ∈ {0,…,9}`.\n",
    "- **Goal**: learn a function $f_θ : ℝ^{784} → ℝ^{10}$ that predicts the correct digit.\n",
    "\n",
    "> **Why flatten?** Convolutional nets (CNNs) exploit spatial structure; an MLP doesn’t. Here we use a simpler **fully-connected** network that treats each pixel as a feature.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Model: Multilayer Perceptron (MLP)\n",
    "\n",
    "An MLP stacks **linear layers** with **nonlinear activations**:\n",
    "\n",
    "$$ x (784) → Linear(784→300) → ReLU → Linear(300→300) → ReLU → Linear(300→10) → logits $$\n",
    "\n",
    "- **Linear layer**: `h = W x + b`.\n",
    "- **ReLU**: `ReLU(z) = max(0, z)` adds nonlinearity so the network can learn complex functions.\n",
    "- **Output**: a length-10 vector of **logits** (unnormalized scores), one per class.\n",
    "\n",
    "**Parameter count (for this exact architecture):**\n",
    "- `784×300 + 300 = 235,500`\n",
    "- `300×300 + 300 = 90,300`\n",
    "- `300×10  + 10  = 3,010`  \n",
    "**Total ≈ 328,810 parameters**\n",
    "\n",
    "> **Why logits (not probabilities)?** Computing probabilities via softmax is deferred to the loss function for numerical stability and correct gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) From logits to probabilities\n",
    "\n",
    "For a sample `x`, the network outputs logits $z ∈ ℝ^{10}$. The **softmax** turns logits into probabilities:\n",
    "$$\n",
    "p_k = \\frac{e^{z_k}}{\\sum_{j=1}^{10} e^{z_j}}\n",
    "$$\n",
    "But in PyTorch, **you should not apply softmax before the loss** if you use `CrossEntropyLoss`.\n",
    "\n",
    "> **PyTorch note**: `nn.CrossEntropyLoss(logits, y)` internally does `log_softmax` + `NLLLoss`. Give it **raw logits** and integer labels `y`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Loss: Multi-class cross-entropy\n",
    "\n",
    "For a single example with true class `y`, the cross-entropy loss is:\n",
    "$$\n",
    "\\mathcal{L} = -\\log p_y\n",
    "$$\n",
    "Averaged over the batch, this encourages the model to put high probability mass on the true class.\n",
    "\n",
    "**Useful gradient fact** (with softmax + cross-entropy):\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_k} = p_k - \\mathbb{1}[k=y]\n",
    "$$\n",
    "i.e., the gradient on the logits is simply **(predicted prob − one-hot target)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Optimization: Gradient descent & Adam\n",
    "\n",
    "Training iteratively improves parameters by following the negative gradient of the loss:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\, \\nabla_\\theta \\mathbb{E}_{(x,y)}[\\mathcal{L}(f_\\theta(x), y)]\n",
    "$$\n",
    "\n",
    "- **Mini-batches**: use `DataLoader` to estimate gradients on small batches (e.g., 32 samples) → faster, smoother updates.\n",
    "- **Adam optimizer**: an adaptive variant of SGD that maintains running estimates of first/second moments of gradients. Typical start: `lr=1e-3`.\n",
    "\n",
    "> **Epoch** = one full pass over the training set. Accuracy typically improves over multiple epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Data preprocessing\n",
    "\n",
    "- **`ToTensor()`** scales pixel values from `[0,255]` to `[0,1]`.\n",
    "- **Normalization (recommended)**:\n",
    "  $$\n",
    "  x \\leftarrow \\frac{x - 0.1307}{0.3081}\n",
    "  $$\n",
    "  These are standard MNIST mean/std. Normalization can stabilize and speed convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Evaluation: from logits to predictions\n",
    "\n",
    "- **Predicted class**: $ŷ = argmax_k z_k$ (or `argmax` over probabilities; both equivalent).\n",
    "- **Accuracy**:\n",
    "  $$\n",
    "  \\text{acc} = \\frac{\\#\\{i : \\hat{y}_i = y_i\\}}{N}\n",
    "  $$\n",
    "- **Confusion matrix (optional)**: shows which digits the model confuses (e.g., 4 vs 9).\n",
    "\n",
    "> **Important**: Disable dropout/batch-norm effects and gradients at test time:\n",
    "```python\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    ...\n",
    "```\n",
    "\n",
    "## 8) Overfitting, regularization, and sanity checks\n",
    "\n",
    "- **Overfitting**: training accuracy ≫ test accuracy.  \n",
    "  Remedies: more data, **weight decay** (L2), **dropout**, reduce hidden size, early stopping.\n",
    "- **Underfitting**: both train and test accuracy are low.  \n",
    "  Remedies: train longer, increase model capacity, tune LR.\n",
    "- **Sanity checks**:\n",
    "  - Can the model **overfit a tiny subset** (e.g., 100 samples)? If not, something’s wrong (bugs, LR too low, etc.).\n",
    "  - Does loss decrease over iterations? If not, check LR, device placement, label dtype, and that you’re using logits with `CrossEntropyLoss`.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Device, dtype, and reproducibility\n",
    "\n",
    "- **Device**: keep model and tensors on the same device:\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device); x, y = x.to(device), y.to(device)\n",
    "\n",
    "```\n",
    "- **Seeds** (help reproducibility, not exact determinism across all backends):\n",
    "```python\n",
    "import torch, random, numpy as np\n",
    "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
    "\n",
    "```\n",
    "- **Mixed precision** (optional): torch.cuda.amp.autocast() can speed training on GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65951485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T09:58:25.803030Z",
     "iopub.status.busy": "2025-09-06T09:58:25.801979Z",
     "iopub.status.idle": "2025-09-06T09:58:30.970553Z",
     "shell.execute_reply": "2025-09-06T09:58:30.969360Z"
    },
    "papermill": {
     "duration": 5.174032,
     "end_time": "2025-09-06T09:58:30.972143",
     "exception": false,
     "start_time": "2025-09-06T09:58:25.798111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.6MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 335kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.14MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.01MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_ds = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_ds = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "print(train_ds)\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b09bf",
   "metadata": {
    "papermill": {
     "duration": 0.003887,
     "end_time": "2025-09-06T09:58:30.980312",
     "exception": false,
     "start_time": "2025-09-06T09:58:30.976425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Why are MNIST pixel values between 0 and 1?\n",
    "\n",
    "This happens because we applied **`transforms.ToTensor()`** when creating the dataset.\n",
    "\n",
    "- Original MNIST images are stored as `uint8` with values in **[0, 255]**.\n",
    "- `ToTensor()` converts the image into a `float32` PyTorch tensor and **divides every pixel by 255**.\n",
    "- Result: pixel intensities are now floats in **[0.0, 1.0]**.\n",
    "\n",
    "Examples:\n",
    "- Black pixel → `0/255 = 0.0`\n",
    "- White pixel → `255/255 = 1.0`\n",
    "- Mid-gray pixel → `128/255 ≈ 0.502`\n",
    "\n",
    "---\n",
    "\n",
    "### Why scale to [0,1]?\n",
    "- Neural networks train more stably with inputs in a small, consistent range.\n",
    "- If we left values in `[0,255]`, the optimization could become unstable.\n",
    "\n",
    "---\n",
    "\n",
    "### Optional: Normalization\n",
    "It’s common to normalize MNIST further using its dataset statistics:\n",
    "\n",
    "```python\n",
    "transforms.Normalize((0.1307,), (0.3081,))\n",
    "```\n",
    "* Mean ≈ 0.1307, Std ≈ 0.3081\n",
    "* This shifts and rescales pixels to have mean ≈ 0 and variance ≈ 1.\n",
    "* Helps the optimizer converge faster and more reliably.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e1c5039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T09:58:30.989779Z",
     "iopub.status.busy": "2025-09-06T09:58:30.989378Z",
     "iopub.status.idle": "2025-09-06T09:58:31.313194Z",
     "shell.execute_reply": "2025-09-06T09:58:31.312243Z"
    },
    "papermill": {
     "duration": 0.330626,
     "end_time": "2025-09-06T09:58:31.314981",
     "exception": false,
     "start_time": "2025-09-06T09:58:30.984355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[channels, height, width] :  torch.Size([1, 28, 28])\n",
      "tensor(0.) tensor(1.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7879c513e490>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = train_ds[2]\n",
    "print(y)\n",
    "print(\"[channels, height, width] : \", x.shape)\n",
    "\n",
    "print(x.min(), x.max())\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(x[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec267b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T09:58:31.326300Z",
     "iopub.status.busy": "2025-09-06T09:58:31.325685Z",
     "iopub.status.idle": "2025-09-06T09:58:31.357926Z",
     "shell.execute_reply": "2025-09-06T09:58:31.356690Z"
    },
    "papermill": {
     "duration": 0.039674,
     "end_time": "2025-09-06T09:58:31.359601",
     "exception": false,
     "start_time": "2025-09-06T09:58:31.319927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['0.10', '0.09', '0.10', '0.10', '0.10', '0.09', '0.10', '0.10', '0.10', '0.11']\n"
     ]
    }
   ],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28*28, 300),\n",
    "            torch.nn.ReLU(), \n",
    "            torch.nn.Linear(300, 300),\n",
    "            torch.nn.ReLU(), \n",
    "            torch.nn.Linear(300, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x.reshape(-1,28*28))\n",
    "\n",
    "\n",
    "net = MLP()  \n",
    "x, y = train_ds[0] \n",
    "probs = torch.softmax(net(x), dim=1)\n",
    "print(y)\n",
    "print([f\"{p:.2f}\" for p in probs[0].tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0425bc",
   "metadata": {
    "papermill": {
     "duration": 0.004203,
     "end_time": "2025-09-06T09:58:31.368460",
     "exception": false,
     "start_time": "2025-09-06T09:58:31.364257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Understanding `CrossEntropyLoss`, logits, predictions, and labels\n",
    "\n",
    "### Logits\n",
    "- The output of the model before any softmax is called **logits**.  \n",
    "- For MNIST, the shape is `(batch_size, 10)` because there are 10 digit classes.  \n",
    "- Each row contains unnormalized scores, which can be any real numbers.\n",
    "\n",
    "### Labels\n",
    "- The true targets (`y`) are integer class indices, **not one-hot vectors**.  \n",
    "- Example: if the true digit is \"3\", the label is simply `3`.\n",
    "\n",
    "### How `CrossEntropyLoss` works\n",
    "- In PyTorch, `nn.CrossEntropyLoss` expects:\n",
    "  - **logits** of shape `(N, C)` where `N` = batch size and `C` = number of classes.  \n",
    "  - **labels** of shape `(N,)` containing integers in `[0, C-1]`.  \n",
    "- Internally it applies:\n",
    "  1. `log_softmax` to convert logits into log-probabilities.  \n",
    "  2. Negative log-likelihood loss comparing these log-probabilities with the true labels.\n",
    "\n",
    "This means **you should not apply softmax before passing logits to the loss**.  \n",
    "Doing so would double-apply softmax and give incorrect results.\n",
    "\n",
    "### Predictions\n",
    "- For accuracy, we turn logits into predicted class indices using `argmax` along the class dimension:\n",
    "  \n",
    "      preds = logits.argmax(dim=1)\n",
    "\n",
    "- This gives a tensor of shape `(N,)` with the most likely class per sample.\n",
    "\n",
    "---\n",
    "\n",
    "### Recap\n",
    "- **Logits** = raw model outputs (any real values).  \n",
    "- **Labels** = integer class indices (0–9 for MNIST).  \n",
    "- **CrossEntropyLoss** = combines softmax + negative log-likelihood, so you give it logits + labels directly.  \n",
    "- **Predictions** = `argmax` over logits when you want to evaluate accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40dba82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T09:58:31.378304Z",
     "iopub.status.busy": "2025-09-06T09:58:31.377964Z",
     "iopub.status.idle": "2025-09-06T09:58:45.893202Z",
     "shell.execute_reply": "2025-09-06T09:58:45.892149Z"
    },
    "papermill": {
     "duration": 14.522049,
     "end_time": "2025-09-06T09:58:45.894739",
     "exception": false,
     "start_time": "2025-09-06T09:58:31.372690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:14<00:00, 129.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=32, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "net = MLP()\n",
    "optimizer = torch.optim.Adam(\n",
    "    net.parameters(), \n",
    "    lr=0.001\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for x,y in tqdm(train_dl):\n",
    "    predicted_y = net(x)\n",
    "    loss = loss_fn(predicted_y, y) \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755d383d",
   "metadata": {
    "papermill": {
     "duration": 0.010353,
     "end_time": "2025-09-06T09:58:45.917393",
     "exception": false,
     "start_time": "2025-09-06T09:58:45.907040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Collecting predictions and labels with `.tolist()`\n",
    "\n",
    "In the evaluation loop we often see:\n",
    "\n",
    "    preds.extend(y_model.argmax(dim=1).tolist())\n",
    "    actuals.extend(y.tolist())\n",
    "\n",
    "- `y_model.argmax(dim=1)` gives a tensor of predicted class indices for the batch  \n",
    "  (for example: tensor([3, 1, 7, 4]) if the batch size is 4).  \n",
    "- `y` is a tensor of true labels for that batch  \n",
    "  (for example: tensor([3, 0, 7, 4])).  \n",
    "- `.tolist()` converts these tensors into regular Python lists:  \n",
    "  - [3, 1, 7, 4]  \n",
    "  - [3, 0, 7, 4]  \n",
    "\n",
    "### Why use `.tolist()`?\n",
    "- `list.extend()` expects a Python iterable (like a list), not a tensor.  \n",
    "- Without `.tolist()`, you might get an error or end up with a list of tensors.  \n",
    "- With `.tolist()`, `preds` and `actuals` become plain lists of integers, which are easy to compare later.\n",
    "\n",
    "---\n",
    "\n",
    "### Alternative\n",
    "We could also keep everything as tensors and concatenate at the end:\n",
    "\n",
    "    all_preds = torch.cat(pred_batches)\n",
    "    all_actuals = torch.cat(label_batches)\n",
    "    accuracy = (all_preds == all_actuals).float().mean()\n",
    "\n",
    "Both methods work; converting to lists is just a simpler, teaching-friendly approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3064812a",
   "metadata": {
    "papermill": {
     "duration": 0.010155,
     "end_time": "2025-09-06T09:58:45.938023",
     "exception": false,
     "start_time": "2025-09-06T09:58:45.927868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Logits, Softmax, and Argmax\n",
    "\n",
    "### Logits\n",
    "- The raw outputs of the network’s final linear layer.  \n",
    "- Shape: `(batch_size, num_classes)` → for MNIST this is `(N, 10)`.  \n",
    "- Values can be any real numbers (positive, negative, large, or small).\n",
    "\n",
    "Example (logits for one sample):\n",
    "- [2.1, -0.3, 0.7, 4.5, 1.2, -1.0, 0.2, 3.0, -0.5, 0.8]\n",
    "\n",
    "---\n",
    "\n",
    "### From logits to probabilities (Softmax)\n",
    "- **Softmax** transforms logits into probabilities in `[0, 1]` that sum to 1:\n",
    "$$\n",
    "  p_k = \\frac{exp(z_k)}{Σ_j exp(z_j)}\n",
    "$$\n",
    "- Example (softmax of the logits above):\n",
    "  - [0.07, 0.01, 0.03, 0.65, 0.05, 0.01, 0.02, 0.13, 0.01, 0.03]\n",
    "- Now we can read this as: “65% chance it’s a 3, 13% chance it’s a 7, etc.”\n",
    "\n",
    "**Important**: do not apply softmax before passing logits to `CrossEntropyLoss`, because the loss applies `log_softmax` internally.\n",
    "\n",
    "---\n",
    "\n",
    "### From probabilities to predicted labels (Argmax)\n",
    "- **Argmax** picks the index of the largest value:\n",
    "  - Can be applied to logits or probabilities (result is the same).\n",
    "- Example:\n",
    "  - Maximum probability = 0.65 at index 3\n",
    "  - Predicted class = 3\n",
    "\n",
    "---\n",
    "\n",
    "### Recap\n",
    "- **Logits** = raw network outputs.  \n",
    "- **Softmax** = converts logits into human-readable probabilities.  \n",
    "- **Argmax** = selects the most likely class index (the prediction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "821cb8a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T09:58:45.960186Z",
     "iopub.status.busy": "2025-09-06T09:58:45.959829Z",
     "iopub.status.idle": "2025-09-06T09:58:47.235332Z",
     "shell.execute_reply": "2025-09-06T09:58:47.234484Z"
    },
    "papermill": {
     "duration": 1.288678,
     "end_time": "2025-09-06T09:58:47.236927",
     "exception": false,
     "start_time": "2025-09-06T09:58:45.948249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 96.77%\n"
     ]
    }
   ],
   "source": [
    "test_dl = torch.utils.data.DataLoader(\n",
    "    test_ds, \n",
    "    batch_size=32, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "preds = []\n",
    "actuals = []\n",
    "\n",
    "for x,y in test_dl:\n",
    "    y_model = net(x)\n",
    "    preds.extend(y_model.argmax(dim=1).tolist())\n",
    "    actuals.extend(y.tolist())\n",
    "\n",
    "\n",
    "n_correct = 0\n",
    "n_total = 0\n",
    "for p, a in zip(preds, actuals):\n",
    "    if p == a: n_correct += 1\n",
    "    n_total += 1 \n",
    "\n",
    "print(f\"accuracy: {n_correct/n_total * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3125ed5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T09:58:47.260600Z",
     "iopub.status.busy": "2025-09-06T09:58:47.260241Z",
     "iopub.status.idle": "2025-09-06T09:58:47.272760Z",
     "shell.execute_reply": "2025-09-06T09:58:47.271786Z"
    },
    "papermill": {
     "duration": 0.026221,
     "end_time": "2025-09-06T09:58:47.274261",
     "exception": false,
     "start_time": "2025-09-06T09:58:47.248040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9677)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = torch.tensor(preds)\n",
    "actuals = torch.tensor(actuals)\n",
    "\n",
    "(preds == actuals).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd779bd5",
   "metadata": {
    "papermill": {
     "duration": 0.010592,
     "end_time": "2025-09-06T09:58:47.295930",
     "exception": false,
     "start_time": "2025-09-06T09:58:47.285338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vectorized vs Loop-based Accuracy Calculation\n",
    "\n",
    "There are two equivalent ways to compute accuracy:\n",
    "\n",
    "- **Vectorized approach (using tensors):**\n",
    "```python \n",
    "    preds = torch.tensor(preds)  \n",
    "    actuals = torch.tensor(actuals)  \n",
    "\n",
    "    (preds == actuals).float().mean()\n",
    "```\n",
    "\n",
    "Here:\n",
    "- `(preds == actuals)` creates a boolean tensor (True/False for each comparison).\n",
    "- `.float()` converts True → 1.0 and False → 0.0.\n",
    "- `.mean()` gives the average fraction of correct predictions = accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "- **Loop-based approach (explicit counting):**\n",
    "```python\n",
    "    n_correct = 0  \n",
    "    n_total = 0  \n",
    "    for p, a in zip(preds, actuals):  \n",
    "        if p == a:  \n",
    "            n_correct += 1  \n",
    "        n_total += 1  \n",
    "\n",
    "    accuracy = n_correct / n_total\n",
    "```\n",
    "This does the same thing by manually counting matches.\n",
    "\n",
    "---\n",
    "\n",
    "### Recap\n",
    "- Both methods give the same result.  \n",
    "- The tensor-based (vectorized) version is more concise and efficient.  \n",
    "- The loop version is more explicit and may be easier to understand when starting out.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 41.671224,
   "end_time": "2025-09-06T09:58:50.009400",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-06T09:58:08.338176",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
