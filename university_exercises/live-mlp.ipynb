{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "091cc16c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-06T07:28:09.805428Z",
     "iopub.status.busy": "2025-09-06T07:28:09.805107Z",
     "iopub.status.idle": "2025-09-06T07:28:21.668779Z",
     "shell.execute_reply": "2025-09-06T07:28:21.667797Z"
    },
    "papermill": {
     "duration": 11.869439,
     "end_time": "2025-09-06T07:28:21.670466",
     "exception": false,
     "start_time": "2025-09-06T07:28:09.801027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15405feb",
   "metadata": {
    "papermill": {
     "duration": 0.002006,
     "end_time": "2025-09-06T07:28:21.675239",
     "exception": false,
     "start_time": "2025-09-06T07:28:21.673233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MLP for MNIST Classification\n",
    "\n",
    "This notebook trains a **Multilayer Perceptron (MLP)** to classify **MNIST** digits (0–9). Below is a concise theory guide to frame each step of the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Problem setup\n",
    "\n",
    "- **Inputs**: grayscale images $x ∈ ℝ^{1×28×28}$ (values in `[0,1]` after `ToTensor`).\n",
    "- **Flattening**: we reshape to a vector $x_{vec} ∈ ℝ^{784}$ (since `28×28=784`) before feeding the MLP.\n",
    "- **Targets**: integer class labels `y ∈ {0,…,9}`.\n",
    "- **Goal**: learn a function $f_θ : ℝ^{784} → ℝ^{10}$ that predicts the correct digit.\n",
    "\n",
    "> **Why flatten?** Convolutional nets (CNNs) exploit spatial structure; an MLP doesn’t. Here we use a simpler **fully-connected** network that treats each pixel as a feature.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Model: Multilayer Perceptron (MLP)\n",
    "\n",
    "An MLP stacks **linear layers** with **nonlinear activations**:\n",
    "\n",
    "$$ x (784) → Linear(784→300) → ReLU → Linear(300→300) → ReLU → Linear(300→10) → logits $$\n",
    "\n",
    "- **Linear layer**: `h = W x + b`.\n",
    "- **ReLU**: `ReLU(z) = max(0, z)` adds nonlinearity so the network can learn complex functions.\n",
    "- **Output**: a length-10 vector of **logits** (unnormalized scores), one per class.\n",
    "\n",
    "**Parameter count (for this exact architecture):**\n",
    "- `784×300 + 300 = 235,500`\n",
    "- `300×300 + 300 = 90,300`\n",
    "- `300×10  + 10  = 3,010`  \n",
    "**Total ≈ 328,810 parameters**\n",
    "\n",
    "> **Why logits (not probabilities)?** Computing probabilities via softmax is deferred to the loss function for numerical stability and correct gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) From logits to probabilities\n",
    "\n",
    "For a sample `x`, the network outputs logits $z ∈ ℝ^{10}$. The **softmax** turns logits into probabilities:\n",
    "$$\n",
    "p_k = \\frac{e^{z_k}}{\\sum_{j=1}^{10} e^{z_j}}\n",
    "$$\n",
    "But in PyTorch, **you should not apply softmax before the loss** if you use `CrossEntropyLoss`.\n",
    "\n",
    "> **PyTorch note**: `nn.CrossEntropyLoss(logits, y)` internally does `log_softmax` + `NLLLoss`. Give it **raw logits** and integer labels `y`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Loss: Multi-class cross-entropy\n",
    "\n",
    "For a single example with true class `y`, the cross-entropy loss is:\n",
    "$$\n",
    "\\mathcal{L} = -\\log p_y\n",
    "$$\n",
    "Averaged over the batch, this encourages the model to put high probability mass on the true class.\n",
    "\n",
    "**Useful gradient fact** (with softmax + cross-entropy):\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_k} = p_k - \\mathbb{1}[k=y]\n",
    "$$\n",
    "i.e., the gradient on the logits is simply **(predicted prob − one-hot target)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Optimization: Gradient descent & Adam\n",
    "\n",
    "Training iteratively improves parameters by following the negative gradient of the loss:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\, \\nabla_\\theta \\mathbb{E}_{(x,y)}[\\mathcal{L}(f_\\theta(x), y)]\n",
    "$$\n",
    "\n",
    "- **Mini-batches**: use `DataLoader` to estimate gradients on small batches (e.g., 32 samples) → faster, smoother updates.\n",
    "- **Adam optimizer**: an adaptive variant of SGD that maintains running estimates of first/second moments of gradients. Typical start: `lr=1e-3`.\n",
    "\n",
    "> **Epoch** = one full pass over the training set. Accuracy typically improves over multiple epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Data preprocessing\n",
    "\n",
    "- **`ToTensor()`** scales pixel values from `[0,255]` to `[0,1]`.\n",
    "- **Normalization (recommended)**:\n",
    "  $$\n",
    "  x \\leftarrow \\frac{x - 0.1307}{0.3081}\n",
    "  $$\n",
    "  These are standard MNIST mean/std. Normalization can stabilize and speed convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Evaluation: from logits to predictions\n",
    "\n",
    "- **Predicted class**: $ŷ = argmax_k z_k$ (or `argmax` over probabilities; both equivalent).\n",
    "- **Accuracy**:\n",
    "  $$\n",
    "  \\text{acc} = \\frac{\\#\\{i : \\hat{y}_i = y_i\\}}{N}\n",
    "  $$\n",
    "- **Confusion matrix (optional)**: shows which digits the model confuses (e.g., 4 vs 9).\n",
    "\n",
    "> **Important**: Disable dropout/batch-norm effects and gradients at test time:\n",
    "```python\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    ...\n",
    "```\n",
    "\n",
    "## 8) Overfitting, regularization, and sanity checks\n",
    "\n",
    "- **Overfitting**: training accuracy ≫ test accuracy.  \n",
    "  Remedies: more data, **weight decay** (L2), **dropout**, reduce hidden size, early stopping.\n",
    "- **Underfitting**: both train and test accuracy are low.  \n",
    "  Remedies: train longer, increase model capacity, tune LR.\n",
    "- **Sanity checks**:\n",
    "  - Can the model **overfit a tiny subset** (e.g., 100 samples)? If not, something’s wrong (bugs, LR too low, etc.).\n",
    "  - Does loss decrease over iterations? If not, check LR, device placement, label dtype, and that you’re using logits with `CrossEntropyLoss`.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Device, dtype, and reproducibility\n",
    "\n",
    "- **Device**: keep model and tensors on the same device:\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device); x, y = x.to(device), y.to(device)\n",
    "\n",
    "```\n",
    "- **Seeds** (help reproducibility, not exact determinism across all backends):\n",
    "```python\n",
    "import torch, random, numpy as np\n",
    "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
    "\n",
    "```\n",
    "- **Mixed precision** (optional): torch.cuda.amp.autocast() can speed training on GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a714f",
   "metadata": {
    "papermill": {
     "duration": 0.001886,
     "end_time": "2025-09-06T07:28:21.679239",
     "exception": false,
     "start_time": "2025-09-06T07:28:21.677353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22734799",
   "metadata": {
    "papermill": {
     "duration": 0.0018,
     "end_time": "2025-09-06T07:28:21.683000",
     "exception": false,
     "start_time": "2025-09-06T07:28:21.681200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad175b6",
   "metadata": {
    "papermill": {
     "duration": 0.001869,
     "end_time": "2025-09-06T07:28:21.686828",
     "exception": false,
     "start_time": "2025-09-06T07:28:21.684959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a523e",
   "metadata": {
    "papermill": {
     "duration": 0.001717,
     "end_time": "2025-09-06T07:28:21.690438",
     "exception": false,
     "start_time": "2025-09-06T07:28:21.688721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c75ce",
   "metadata": {
    "papermill": {
     "duration": 0.00174,
     "end_time": "2025-09-06T07:28:21.694048",
     "exception": false,
     "start_time": "2025-09-06T07:28:21.692308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb95536",
   "metadata": {
    "papermill": {
     "duration": 0.001879,
     "end_time": "2025-09-06T07:28:21.698294",
     "exception": false,
     "start_time": "2025-09-06T07:28:21.696415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf9b40",
   "metadata": {
    "papermill": {
     "duration": 0.001868,
     "end_time": "2025-09-06T07:28:21.702148",
     "exception": false,
     "start_time": "2025-09-06T07:28:21.700280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.46363,
   "end_time": "2025-09-06T07:28:23.529894",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-06T07:28:04.066264",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
